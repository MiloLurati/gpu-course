{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reduction\n",
    "\n",
    "The goal of this exercise is to teach you about shared memory and parallel programming on a thread block level in the CUDA programming model.\n",
    "\n",
    "Please follow these steps:\n",
    "\n",
    "**Step 1.** Read through the entire notebook and run all cells at least once\n",
    "\n",
    "**Step 2.** Implement the missing part of the kernel code, following the instructions in the comments. The goal is that shared memory is used to sum the per-thread partial sums into a single per-thread-block partial sum\n",
    "\n",
    "Hints:\n",
    "* The kernel uses a so-called grid strided loop, where all threads in the grid cooperatively iterate over the problem domain. Therefore, the number of thread blocks does not depend on the size of the to be summed array. All threads from all blocks first iterate (collectively) over the problem size (n) to obtain a per-thread partial sum.\n",
    "* Within the thread block the per-thread partial sums are to be combined into a single per-thread-block partial sum.\n",
    "* Each thread block stores its partial sum to ``out_array[blockIdx.x]``\n",
    "* The kernel is called twice, the second kernel is executed with only one thread block to combine all per-block partial sums to a single, global sum for the whole input array\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pycuda.driver as drv\n",
    "from pycuda.compiler import SourceModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize pycuda and create a device context\n",
    "drv.init()\n",
    "context = drv.Device(0).make_context()\n",
    "\n",
    "#get compute capability for compiling CUDA kernels\n",
    "devprops = { str(k): v for (k, v) in context.get_device().get_attributes().items() }\n",
    "cc = str(devprops['COMPUTE_CAPABILITY_MAJOR']) + str(devprops['COMPUTE_CAPABILITY_MINOR'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Executing the following cell block writes its contents to the file \"reduction.cu\". Please read the comments to see what you need you to do to complete the implementation of this kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile reduction.cu\n",
    "\n",
    "#define block_size_x 256\n",
    "\n",
    "__global__ void reduce_kernel(float *out_array, float *in_array, int n) {\n",
    "\n",
    "    int ti = threadIdx.x;\n",
    "    int x = blockIdx.x * block_size_x + threadIdx.x;\n",
    "    int step_size = gridDim.x * block_size_x;\n",
    "    float sum = 0.0f;\n",
    "\n",
    "    //cooperatively (with all threads in all thread blocks) iterate over input array\n",
    "    for (int i=x; i<n; i+=step_size) {\n",
    "        sum += in_array[i];\n",
    "    }\n",
    "\n",
    "    //at this point we have reduced the number of values to be summed from n to\n",
    "    //the total number of threads in all thread blocks combined\n",
    "\n",
    "    //the goal is now to reduce the values within each thread block to a single\n",
    "    //value per thread block for this we will need shared memory\n",
    "\n",
    "    //declare shared memory array, how much shared memory do we need?\n",
    "    //__shared__ float ...;\n",
    "\n",
    "    //make every thread store its thread-local sum to the array in shared memory\n",
    "    //... = sum;\n",
    "    \n",
    "    //now let's call syncthreads() to make sure all threads have finished\n",
    "    //storing their local sums to shared memory\n",
    "    __syncthreads();\n",
    "\n",
    "    //now this interesting looking loop will do the following:\n",
    "    //it iterates over the block_size_x with the following values for s:\n",
    "    //if block_size_x is 256, 's' will be powers of 2 from 128, 64, 32, down to 1.\n",
    "    //these decreasing offsets can be used to reduce the number\n",
    "    //of values within the thread block in only a few steps.\n",
    "    #pragma unroll\n",
    "    for (unsigned int s=block_size_x/2; s>0; s/=2) {\n",
    "\n",
    "        //you are to write the code inside this loop such that\n",
    "        //threads will add the sums of other threads that are 's' away\n",
    "        //do this iteratively such that together the threads compute the\n",
    "        //sum of all thread-local sums \n",
    "\n",
    "        //use shared memory to access the values of other threads\n",
    "        //and store the new value in shared memory to be used in the next round\n",
    "        //be careful that values that should be read are\n",
    "        //not overwritten before they are read\n",
    "        //make sure to call __syncthreads() when needed\n",
    "    }\n",
    "\n",
    "    //write back one value per thread block\n",
    "    if (ti == 0) {\n",
    "        //out_array[blockIdx.x] = ;  //store the per-thread block reduced value to global memory\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following prepares the input and output data of our kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setup kernel launch parameters\n",
    "block_size_x = 256\n",
    "num_blocks = 2048\n",
    "threads = (block_size_x, 1, 1)\n",
    "grid = (num_blocks, 1, 1)\n",
    "\n",
    "#create input and output data\n",
    "n = np.int32(5e7)\n",
    "in_array = np.random.randn(n).astype(np.float32) + 0.00000001\n",
    "out_array = np.zeros(num_blocks).astype(np.float32)\n",
    "\n",
    "#allocate GPU memory and move data to the GPU\n",
    "args = [out_array, in_array]\n",
    "gpu_args = []\n",
    "for arg in args:\n",
    "    gpu_args.append(drv.mem_alloc(arg.nbytes))\n",
    "    drv.memcpy_htod(gpu_args[-1], arg)\n",
    "gpu_args.append(n)\n",
    "gpu_args2 = [gpu_args[0], gpu_args[0], np.int32(num_blocks)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compute a reference answer in Python to check if our GPU kernel returns the correct result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "#compute reference sum in Python\n",
    "npsum = np.sum(in_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell compiles and runs the kernel and checks if the output is correct. This is all in one cell so that you only need to rerun this cell after modifying the kernel code in the cell above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read kernel into string\n",
    "with open('reduction.cu', 'r') as f:\n",
    "    kernel_string = f.read()\n",
    "\n",
    "#compile the kernel\n",
    "reduce_kernel = SourceModule(kernel_string, arch='compute_' + cc, code='sm_' + cc,\n",
    "                cache_dir=False).get_function(\"reduce_kernel\")\n",
    "\n",
    "#clear the GPU output array for correctness checks\n",
    "out_array = np.zeros(num_blocks).astype(np.float32)\n",
    "drv.memcpy_htod(gpu_args[0], out_array)\n",
    "\n",
    "#make sure all previous operations have completed\n",
    "context.synchronize()\n",
    "start = drv.Event()\n",
    "end = drv.Event()\n",
    "\n",
    "#run the kernels\n",
    "start.record()\n",
    "reduce_kernel(*gpu_args, block=threads, grid=grid, stream=None, shared=0)\n",
    "reduce_kernel(*gpu_args2, block=threads, grid=(1,1), stream=None, shared=0)\n",
    "end.record()\n",
    "context.synchronize()\n",
    "\n",
    "print(\"reduction_kernel took\", end.time_since(start), \"ms.\")\n",
    "\n",
    "#copy output data back from GPU\n",
    "gpu_sum = np.zeros(1).astype(np.float32)\n",
    "drv.memcpy_dtoh(gpu_sum, gpu_args[0])\n",
    "    \n",
    "print(\"PASSED\" if np.absolute(npsum - gpu_sum) < 10.0 else \"FAILED\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
